{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA evaluation notebook\n",
    "\n",
    "We use an interactive notebook to evaluate our summarization models using our LDA model.\n",
    "\n",
    "The notebook uses custom-modules defined in other files, but to prevent ourselves from re-loading the data during training, it is easier to use a notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resource paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "\n",
    "bart_path = f'{cwd}/bart_output_.json'\n",
    "t5_path = f'{cwd}/t5_output_.json'\n",
    "pegasus_path = f'{cwd}/pegasus_output_.json'\n",
    "\n",
    "model_path = f'{cwd}/model/grid-xxx'\n",
    "tf_idf_path = f'{cwd}/tf_idf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-computed resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "from lda_model import LdaModel\n",
    "\n",
    "tf_idf = TfidfModel.load(tf_idf_path)\n",
    "lda = LdaModel.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "We use our self-made JSON file that stores the original article and abstract (part of the dataset) and the BART model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from glob import glob\n",
    "\n",
    "def read_output(description, data_path, summary_key):\n",
    "    data = []\n",
    "    data_files = glob(data_path)\n",
    "    for file in data_files:\n",
    "        with open(file) as fin:\n",
    "            data += json.load(fin)\n",
    "\n",
    "    return {\n",
    "        'description': description,\n",
    "        'articles': [doc['article'] for doc in data],\n",
    "        'abstracts': [doc['abstract'] for doc in data],\n",
    "        'summaries': [doc[summary_key] for doc in data]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading output...')\n",
    "\n",
    "bart_output = read_output('BART', bart_path, 'summary')\n",
    "print('BART: ', len(bart_output['articles']))\n",
    "\n",
    "t5_output = read_output('T5', t5_path, 't5_abstract')\n",
    "print('T5: ', len(t5_output['articles']))\n",
    "\n",
    "pegasus_output = read_output('Pegasus', pegasus_path, 'pegasus_abstract')\n",
    "print('Pegasus: ', len(pegasus_output['articles']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and pre-process test data\n",
    "\n",
    "The LDA model expects a BOW input (in our case TF-IDF), not strings. Hence we need to convert each of the texts into the expected format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_preprocessed import PreProcessor\n",
    "from generate_bow import BowProcessor\n",
    "from generate_tf_idf import TfIdfProcessor\n",
    "\n",
    "pp_processor = PreProcessor()\n",
    "bow_processor = BowProcessor(lda.dictionary)\n",
    "tf_idf_processor = TfIdfProcessor(tf_idf)\n",
    "\n",
    "def process_output(output):\n",
    "    return {\n",
    "        'description': output['description'],\n",
    "        'articles': tf_idf_processor(bow_processor(pp_processor(output['articles']))),\n",
    "        'abstracts': tf_idf_processor(bow_processor(pp_processor(output['abstracts']))),\n",
    "        'summaries': tf_idf_processor(bow_processor(pp_processor(output['summaries'])))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_tf_idf = process_output(bart_output)\n",
    "t5_tf_idf = process_output(t5_output)\n",
    "pegasus_tf_idf = process_output(pegasus_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate topic retention for each doc and calculate distances\n",
    "\n",
    "For every original article, we have two gists: one human-made (abstract) and one computer-made (summary).  \n",
    "We calculate the distance between the two pair (original, abstract) and (original, summary), and examine which one retains topics better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lda_eval import LdaEvaluator\n",
    "\n",
    "evaluator = LdaEvaluator(lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "def lda_compare(collection):\n",
    "    description = collection['description']\n",
    "    articles_tf_idf = collection['articles']\n",
    "    abstracts_tf_idf = collection['abstracts']\n",
    "    summaries_tf_idf = collection['summaries']\n",
    "\n",
    "    human_stats = []\n",
    "    comp_stats = []\n",
    "    for article, abstract, summary in zip(articles_tf_idf, abstracts_tf_idf, summaries_tf_idf):\n",
    "        human_stats.append(evaluator(article, abstract))\n",
    "        comp_stats.append(evaluator(article, summary))\n",
    "\n",
    "    human_distances = [x['divergence'] for x in human_stats]\n",
    "    comp_distances = [x['divergence'] for x in comp_stats]\n",
    "\n",
    "    human_avg = mean(human_distances)\n",
    "    human_better = sum([h > c for h,c in zip(human_distances, comp_distances)])\n",
    "\n",
    "    comp_avg = mean(comp_distances)\n",
    "    comp_better = sum([h < c for h,c in zip(human_distances, comp_distances)])\n",
    "\n",
    "    print(f'-- {description} ---------------------------------------')\n",
    "    print(f'Model average: {comp_avg}')\n",
    "    print(f'Model is better: {comp_better}')\n",
    "    print(f'Human average: {human_avg}')\n",
    "    print(f'Human is better: {human_better}')\n",
    "\n",
    "    return human_stats, comp_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_compare(bart_tf_idf)\n",
    "lda_compare(t5_tf_idf)\n",
    "lda_compare(pegasus_tf_idf)\n",
    "\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate ROUGE metric\n",
    "\n",
    "For every output produced by a model, calculate the industry standard ROUGE metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "rouge = Rouge()\n",
    "\n",
    "def rouge_compare(input):\n",
    "    description = input['description']\n",
    "    abstracts = input['abstracts']\n",
    "    summaries = input['summaries']\n",
    "\n",
    "    score = rouge.get_scores(summaries, abstracts, avg=True)\n",
    "\n",
    "    print(f'-- {description} ---------------------------------------')\n",
    "    print(json.dumps(score['rouge-1']))\n",
    "    print(json.dumps(score['rouge-2']))\n",
    "    print(json.dumps(score['rouge-l']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_compare(bart_output)\n",
    "rouge_compare(t5_output)\n",
    "rouge_compare(pegasus_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate discrete ROUGE vs. TRRE samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model to examine\n",
    "model_output = bart_output\n",
    "model_tf_idf = bart_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_stats, comp_stats = lda_compare(model_tf_idf)\n",
    "articles, abstracts, summaries = model_output['articles'], model_output['abstracts'], model_output['summaries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_tf_idf = model_tf_idf['articles']\n",
    "abstracts_tf_idf = model_tf_idf['abstracts']\n",
    "summaries_tf_idf = model_tf_idf['summaries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "\n",
    "# Estimate what is a \"high\" divergence, so that we can try and analyze those\n",
    "div = [abs(h['divergence'] - c['divergence']) for h,c in zip(human_stats, comp_stats)]\n",
    "high_div = floor(max(div))\n",
    "high_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (human, comp, article, abstract, summary) in enumerate(zip(human_stats, comp_stats, articles, abstracts, summaries)):\n",
    "    article_topics = human['original_topics']\n",
    "\n",
    "    h_topics = human['summary_topics']\n",
    "    h_div = human['divergence']\n",
    "\n",
    "    c_topics = comp['summary_topics']\n",
    "    c_div = comp['divergence']\n",
    "\n",
    "    if abs(h_div - c_div) < high_div:\n",
    "        continue\n",
    "\n",
    "    _, h_topics_per_word, h_phi = lda.model.get_document_topics(abstracts_tf_idf[i], per_word_topics=True)\n",
    "    h_topics_per_word = [(x, y[:3]) for x,y in h_topics_per_word]\n",
    "    h_phi = [(x, sorted(y, reverse=True, key=lambda tup: tup[1])[:3]) for x,y in h_phi]\n",
    "\n",
    "    _, c_topics_per_word, c_phi = lda.model.get_document_topics(summaries_tf_idf[i], per_word_topics=True)\n",
    "    c_topics_per_word = [(x, y[:3]) for x,y in c_topics_per_word]\n",
    "    c_phi = [(x, sorted(y, reverse=True, key=lambda tup: tup[1])[:3]) for x,y in c_phi]\n",
    "\n",
    "    score = rouge.get_scores(summary, abstract)\n",
    "\n",
    "    print(f'#{i}')\n",
    "    print('>>> ROUGE:\\n', score[0])\n",
    "    print('>>> Article topics:\\n', article_topics[:3])\n",
    "    print('>>> Human divergence:\\n', h_div)\n",
    "    print('>>> Human topics:\\n', h_topics[:3])\n",
    "    print('>>> Human per-word topics:\\n', h_topics_per_word)\n",
    "    print('>>> Human phi:\\n', h_phi)\n",
    "    print('>>> Comp divergence:\\n', c_div)\n",
    "    print('>>> Comp topics:\\n', c_topics[:3])\n",
    "    print('>>> Comp per-word topics:\\n', c_topics_per_word)\n",
    "    print('>>> Comp phi:\\n', c_phi)\n",
    "    print('>>> Article:\\n', article)\n",
    "    print('>>> Abstract:\\n', abstract)\n",
    "    print('>>> Summary:\\n', summary)\n",
    "\n",
    "    h_top_topic = h_topics[0][0]\n",
    "    c_top_topic = c_topics[0][0]\n",
    "    print('Topic #', h_top_topic, '\\n', lda.model.print_topic(h_top_topic))\n",
    "    if h_top_topic != c_top_topic:\n",
    "        print('Topic #', c_top_topic, '\\n', lda.model.print_topic(c_top_topic))\n",
    "\n",
    "    print('========================================')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4fe221a22f9802ae2a050549df6cbcd7264e25be7971b2914a087a6bea67c1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
