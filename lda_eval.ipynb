{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA evaluation notebook\n",
    "\n",
    "We use an interactive notebook to evaluate our summarization models using our LDA model.\n",
    "\n",
    "The notebook uses custom-modules defined in other files, but to prevent ourselves from re-loading the data during training, it is easier to use a notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from logging import config\n",
    "config.fileConfig('./logging.conf')\n",
    "\n",
    "def pp(*args, **kwargs):\n",
    "    logging.info(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resource paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "\n",
    "data_path = f'{cwd}/bart_output.json'\n",
    "model_path = f'{cwd}/model/grid-xxx'\n",
    "tf_idf_path = f'{cwd}/tf_idf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-computed resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "\n",
    "tf_idf = TfidfModel.load(tf_idf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lda_model import LdaModel\n",
    "\n",
    "lda = LdaModel.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "We use our self-made JSON file that stores the original article and abstract (part of the dataset) and the BART model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(data_path) as fin:\n",
    "    data = json.load(fin)\n",
    "\n",
    "articles = [doc['article'] for doc in data]\n",
    "abstracts = [doc['abstract'] for doc in data]\n",
    "summaries = [doc['bart'] for doc in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and pre-process test data\n",
    "\n",
    "The LDA model expects a BOW input (in our case TF-IDF), not strings. Hence we need to convert each of the texts into the expected format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_preprocessed import PreProcessor\n",
    "from generate_bow import BowProcessor\n",
    "from generate_tf_idf import TfIdfProcessor\n",
    "\n",
    "pp_processor = PreProcessor()\n",
    "bow_processor = BowProcessor(lda.dictionary)\n",
    "tf_idf_processor = TfIdfProcessor(tf_idf)\n",
    "\n",
    "articles_pp = pp_processor(articles)\n",
    "abstracts_pp = pp_processor(abstracts)\n",
    "summaries_pp = pp_processor(summaries)\n",
    "\n",
    "articles_bow = bow_processor(articles_pp)\n",
    "abstracts_bow = bow_processor(abstracts_pp)\n",
    "summaries_bow = bow_processor(summaries_pp)\n",
    "\n",
    "articles_tf_idf = tf_idf_processor(articles_bow)\n",
    "abstracts_tf_idf = tf_idf_processor(abstracts_bow)\n",
    "summaries_tf_idf = tf_idf_processor(summaries_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the topics for each doc and calculate distances\n",
    "\n",
    "For every original article, we have two gists: one human-made (abstract) and one computer-made (summary).  \n",
    "We calculate the distance between the two pair (original, abstract) and (original, summary), and examine which one retains topics better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lda_eval import LdaEvaluator\n",
    "\n",
    "evaluator = LdaEvaluator(lda)\n",
    "\n",
    "human_better = 0\n",
    "comp_better = 0\n",
    "\n",
    "for article, abstract, summary in zip(articles_tf_idf, abstracts_tf_idf, summaries_tf_idf):\n",
    "    human_dist = evaluator.distance(article, abstract)\n",
    "    comp_dist = evaluator.distance(article, summary)\n",
    "    diff = abs(human_dist - comp_dist)\n",
    "    pp(f'{human_dist:.3f}, {comp_dist:.3f} --> {diff:.3f}')\n",
    "    if human_dist < comp_dist:\n",
    "        human_better += 1\n",
    "    else:\n",
    "        comp_better += 1\n",
    "\n",
    "pp('---------------------------------------------------')\n",
    "pp(f'Human [{human_better}] vs. Comp [{comp_better}]')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4fe221a22f9802ae2a050549df6cbcd7264e25be7971b2914a087a6bea67c1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
