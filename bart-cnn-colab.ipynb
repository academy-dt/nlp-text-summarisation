{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/academy-dt/nlp-text-summarisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('./nlp-text-summarisation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git submodule init\n",
    "!git submodule update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sklearn transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = '/content/drive/MyDrive/test_dataset'\n",
    "test_data_chunk = '000'\n",
    "test_data_path = f'{test_dataset}/test_{test_data_chunk}.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "device = 'cuda:0'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\").to(device)\n",
    "pipe = pipeline(\"summarization\", model=model, tokenizer=tokenizer, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generators import get_cnn_dm_both_generator\n",
    "\n",
    "test_data_gen = get_cnn_dm_both_generator(test_data_path)\n",
    "test_data = list(test_data_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "output = []\n",
    "for article, abstract in test_data[:1]:\n",
    "    try:\n",
    "        tokens = tokenizer(article, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        outputs = model(**tokens)\n",
    "        summary = tokenizer.decode(outputs, skip_special_tokens=True)\n",
    "        output.append({\n",
    "            'article': article,\n",
    "            'abstract': abstract,\n",
    "            'summary': summary\n",
    "        })\n",
    "    except Exception as ex:\n",
    "        print(f'\\t ERROR: {str(ex)}')\n",
    "\n",
    "# with open(f'bart_output_{test_data_chunk}.json', 'w') as fout:\n",
    "#     json.dump(output, fout, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4fe221a22f9802ae2a050549df6cbcd7264e25be7971b2914a087a6bea67c1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
